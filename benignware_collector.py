# -*- coding: UTF-8 -*-
'''
function: 爬取开源良性网站
https://pc.qq.com/category/c0.html

'''

import urllib2
from bs4 import BeautifulSoup
import os
import re
import requests
import sys
type = sys.getfilesystemencoding()
from threading import Thread
import time
from multiprocessing.pool import ThreadPool

headers = {
        # "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8",
        # "Accept-Encoding": "gzip, deflate",
        # "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8",
        # "Cache-Control": "no-cache",
        # "Connection": "keep-alive",
        # "Cookie": "_ga=GA1.2.923209739.1545890753",
        # "Host": "malwaredb.malekal.com",
        # "Pragma": "no-cache",
        # "Upgrade-Insecure-Requests": 1,
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/71.0.3578.98 Safari/537.36"
        }


def download(_url):  # 下载函数
    file_name = _url.split("/")[-1]
    file_path = "E:\\benign_file\\"+file_name  # 设置路径和文件名
    if os.path.exists(file_path) == False:
        if (_url == None):  # 地址若为None则跳过
            pass
        result = requests.get(_url, headers=headers)
        data = result.content  # 否则开始下载到本地
        with open(file_path, "wb") as f:
            f.write(data)
            f.close()


def getRemoteFileSize(url, proxy=None):
    '''
    通过content-length头获取远程文件大小
    '''
    opener = urllib2.build_opener()
    if proxy:
        if url.lower().startswith('https://'):
            opener.add_handler(urllib2.ProxyHandler({'https' : proxy}))
        elif url.lower().startswith('http://'):
            opener.add_handler(urllib2.ProxyHandler({'http' : proxy}))
        else:
            opener.add_handler(urllib2.ProxyHandler({'ftp': proxy}))
    try:
        request = urllib2.Request(url)
        request.get_method = lambda: 'HEAD'
        response = opener.open(request)
        response.read()
    except Exception, e:
        # 远程文件不存在
        return 0
    else:
        getfileSize = dict(response.headers).get('content-length', 0)
        filesize = round(float(getfileSize) / 1048576, 2)
        getContentType = dict(response.headers).get('content-type', 0)
        return filesize

count = 0
if __name__=="__main__":
    lst=[]
    # i=0
    # while count<1300 and i<=381:
    for i in range(137,200):
        print(i)
        url="https://pc.qq.com/category/c0-"+str(i)+".html"#c0-1 c0-2.。。
        html = requests.get(url)
        html.encoding = 'utf-8'  # utf-8是根据网页源代码中设置的编码格式来指定的
        soup = BeautifulSoup(html.text, 'lxml')
        file_download=soup.find_all("a",attrs={"class": "btn-download-cat-normal J_select_normal"})
        for j in range(0,len(file_download)-1):
            download_url=file_download[j].get('href')
            print(download_url)
            file_size=getRemoteFileSize(download_url)
            # print(file_size)
            if file_size>0 and file_size<5:
                print("hhh "+download_url)
                download(download_url)
                count+=1
        # if count>=1400:
        #     break

    # print("count: "+ str(count))

    # for item in lst:
    #     # print(item)
    #     download(item)







